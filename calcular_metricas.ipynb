{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import mdpd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interceptamos las respuestas reales contra las predichas\n",
    "df = pd.read_excel(\"ignorar/comprar_resultados.xlsx\")\n",
    "df['columnas realidad'] = df['columnas realidad'].apply(ast.literal_eval)\n",
    "df['columnas ejecuciones'] = df['columnas ejecuciones'].apply(ast.literal_eval)\n",
    "\n",
    "for i, fila in df.iterrows():\n",
    "    if not fila['ejecutar']:\n",
    "        continue\n",
    "    respuesta_real = pd.DataFrame()\n",
    "    \n",
    "    with open(fila['archivo realidad'],'r') as f:\n",
    "        contenido:str = f.read()\n",
    "        inicio = contenido.find(\"INFO: resultado: '''\") - len(\"INFO: resultado: '''\") - 1\n",
    "        fin = contenido.find(\" '''\\nWARNING: }\") - 1\n",
    "        \n",
    "        respuesta_real = mdpd.from_md(contenido[inicio : fin])\n",
    "    \n",
    "    respuestas_predichas = []\n",
    "    with open(fila['archivo ejecuciones'], 'r') as f:\n",
    "        contenido:str = f.read()\n",
    "        \n",
    "        inicios = [inicio.end () for inicio in re.finditer(\"INFO: resultado: '''\", contenido)]\n",
    "        finales = [final.start() for final in re.finditer(\" '''\\nWARNING: }\", contenido)]\n",
    "        tiempos = [final.group(1) for final in re.finditer(r\"INFO: tiempo: (\\d*.\\d*),\", contenido)]\n",
    "        for i in range(len(inicios)):\n",
    "            respuestas_predichas.append(mdpd.from_md(contenido[inicios[i] + 1: finales[i]-1]))\n",
    "    \n",
    "    \n",
    "    resultados = []\n",
    "    \n",
    "    for prediccion, tiempo in zip(respuestas_predichas, tiempos):\n",
    "        \n",
    "        resumen = {}\n",
    "        resumen['Cantidad de respuestas esperadas'] = len(respuesta_real)\n",
    "        resumen['Cantidad de respuesta predichas'] = len(prediccion) \n",
    "        resumen['Tiempo de ejecución'] = tiempo\n",
    "\n",
    "        if prediccion.empty:\n",
    "            prediccion = pd.DataFrame(columns=fila['columnas ejecuciones'])\n",
    "        \n",
    "        # Aqui procesaremos los datos de las predicciones por si tienen\n",
    "        # algun caracter extraño. Caracteres extraños detectados:\n",
    "        #    - \\xa0: Un especie de espacio en blanco que no es el mismo caracter\n",
    "        #            que el verdadero espacio en blanco\n",
    "        \n",
    "        tuplas_prediccion = []\n",
    "        for record in prediccion.itertuples(index=False, name=None):\n",
    "            tupla = []\n",
    "            for i in range(len(fila['columnas ejecuciones'])):\n",
    "                if isinstance(record[i], str):\n",
    "                    tupla.append(record[i].replace(u'\\xa0', u' '))\n",
    "                else:\n",
    "                    tupla.append(record[i])\n",
    "                    \n",
    "            tuplas_prediccion.append(tuple(tupla))\n",
    "        \n",
    "        tuplas_realidad = list(respuesta_real.itertuples(index=False, name=None))\n",
    "        \n",
    "        resumen['Cantidad de respuestas predichas que son correctas'] = len(set(tuplas_realidad).intersection(set(tuplas_prediccion)))\n",
    "        resultados.append(resumen)   \n",
    "    \n",
    "    with open(fila['archivo resultado'], 'w') as f:\n",
    "        for i, resultado in enumerate(resultados):\n",
    "            f.write(f\"\\n@ Metricas de la ejecución {i+1}\\n\")\n",
    "            f.write(f\"metricas: {resultado}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntamos todas las metricas\n",
    "carpeta_metricas = 'resultados\\metricas\\ln\\llama2-uncensored'\n",
    "nombre_archivos = os.listdir(carpeta_metricas)\n",
    "for i, nombre in enumerate(nombre_archivos):\n",
    "    resumenes = []\n",
    "    with open(os.path.join(carpeta_metricas, nombre),'r') as f:\n",
    "        contenido = f.read()\n",
    "        resumenes = [ast.literal_eval(diccionario.group(1)) for diccionario in re.finditer(r'metricas: (.*)\\n',contenido)]\n",
    "    \n",
    "    precision_avg = 0\n",
    "    recall_avg = 0\n",
    "    time_avg = 0\n",
    "    for resumen in resumenes:\n",
    "        if resumen['Cantidad de respuesta predichas'] != 0:\n",
    "            precision_avg += (resumen['Cantidad de respuestas predichas que son correctas'] / resumen['Cantidad de respuesta predichas']) * 1/len(resumenes)\n",
    "        \n",
    "        recall_avg += (resumen['Cantidad de respuestas predichas que son correctas']/ resumen['Cantidad de respuestas esperadas'])* 1/len(resumenes)\n",
    "        \n",
    "        time_avg += float(resumen['Tiempo de ejecución']) * 1/len(resumenes)\n",
    "    \n",
    "    with open(os.path.join(carpeta_metricas, 'resumen.txt'),'a') as f:\n",
    "        f.write(f\"\\n@ Resumen de las metricas de la ejecución {i+1}\\n\")\n",
    "        f.write(f\"Precisión media: {precision_avg}\\nRecall media: {recall_avg}\\ntiempo medio: {time_avg}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evariste-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
